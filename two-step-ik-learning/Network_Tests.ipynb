{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85af370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "072ec4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, h1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "915b11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': torch.FloatTensor(self.data[idx]),\n",
    "            'label': torch.FloatTensor([self.labels[idx]])\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79d6d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../ea-based-nn-ik-solver/datasets/6DoF-6R-Puma260/data_3DoF-3R.csv')\n",
    "data = np.array(data).astype(np.float32)\n",
    "joints = data[:,-3:]\n",
    "pos = data[:,:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "878f9bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7db2472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(joints, pos, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f193ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a5223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "507e3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = joints.shape[1]\n",
    "h1 = 64\n",
    "h2 = 128\n",
    "output_size = pos.shape[1]\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c2bfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, h1, h2, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b058757",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5162\n",
      "Epoch [2/100], Loss: 0.4383\n",
      "Epoch [3/100], Loss: 0.4036\n",
      "Epoch [4/100], Loss: 0.2905\n",
      "Epoch [5/100], Loss: 0.2804\n",
      "Epoch [6/100], Loss: 0.1706\n",
      "Epoch [7/100], Loss: 0.1836\n",
      "Epoch [8/100], Loss: 0.1569\n",
      "Epoch [9/100], Loss: 0.1125\n",
      "Epoch [10/100], Loss: 0.0725\n",
      "Epoch [11/100], Loss: 0.0911\n",
      "Epoch [12/100], Loss: 0.0486\n",
      "Epoch [13/100], Loss: 0.0745\n",
      "Epoch [14/100], Loss: 0.0922\n",
      "Epoch [15/100], Loss: 0.0485\n",
      "Epoch [16/100], Loss: 0.0362\n",
      "Epoch [17/100], Loss: 0.0434\n",
      "Epoch [18/100], Loss: 0.0673\n",
      "Epoch [19/100], Loss: 0.0501\n",
      "Epoch [20/100], Loss: 0.0271\n",
      "Epoch [21/100], Loss: 0.0367\n",
      "Epoch [22/100], Loss: 0.0218\n",
      "Epoch [23/100], Loss: 0.0233\n",
      "Epoch [24/100], Loss: 0.0376\n",
      "Epoch [25/100], Loss: 0.0239\n",
      "Epoch [26/100], Loss: 0.0322\n",
      "Epoch [27/100], Loss: 0.0347\n",
      "Epoch [28/100], Loss: 0.0392\n",
      "Epoch [29/100], Loss: 0.0229\n",
      "Epoch [30/100], Loss: 0.0213\n",
      "Epoch [31/100], Loss: 0.0276\n",
      "Epoch [32/100], Loss: 0.0275\n",
      "Epoch [33/100], Loss: 0.0281\n",
      "Epoch [34/100], Loss: 0.0173\n",
      "Epoch [35/100], Loss: 0.0183\n",
      "Epoch [36/100], Loss: 0.0284\n",
      "Epoch [37/100], Loss: 0.0260\n",
      "Epoch [38/100], Loss: 0.0140\n",
      "Epoch [39/100], Loss: 0.0141\n",
      "Epoch [40/100], Loss: 0.0228\n",
      "Epoch [41/100], Loss: 0.0211\n",
      "Epoch [42/100], Loss: 0.0139\n",
      "Epoch [43/100], Loss: 0.0113\n",
      "Epoch [44/100], Loss: 0.0272\n",
      "Epoch [45/100], Loss: 0.0279\n",
      "Epoch [46/100], Loss: 0.0114\n",
      "Epoch [47/100], Loss: 0.0246\n",
      "Epoch [48/100], Loss: 0.0175\n",
      "Epoch [49/100], Loss: 0.0129\n",
      "Epoch [50/100], Loss: 0.0143\n",
      "Epoch [51/100], Loss: 0.0161\n",
      "Epoch [52/100], Loss: 0.0208\n",
      "Epoch [53/100], Loss: 0.0149\n",
      "Epoch [54/100], Loss: 0.0125\n",
      "Epoch [55/100], Loss: 0.0202\n",
      "Epoch [56/100], Loss: 0.0193\n",
      "Epoch [57/100], Loss: 0.0124\n",
      "Epoch [58/100], Loss: 0.0158\n",
      "Epoch [59/100], Loss: 0.0141\n",
      "Epoch [60/100], Loss: 0.0159\n",
      "Epoch [61/100], Loss: 0.0107\n",
      "Epoch [62/100], Loss: 0.0089\n",
      "Epoch [63/100], Loss: 0.0131\n",
      "Epoch [64/100], Loss: 0.0093\n",
      "Epoch [65/100], Loss: 0.0124\n",
      "Epoch [66/100], Loss: 0.0091\n",
      "Epoch [67/100], Loss: 0.0088\n",
      "Epoch [68/100], Loss: 0.0190\n",
      "Epoch [69/100], Loss: 0.0081\n",
      "Epoch [70/100], Loss: 0.0106\n",
      "Epoch [71/100], Loss: 0.0129\n",
      "Epoch [72/100], Loss: 0.0177\n",
      "Epoch [73/100], Loss: 0.0104\n",
      "Epoch [74/100], Loss: 0.0131\n",
      "Epoch [75/100], Loss: 0.0094\n",
      "Epoch [76/100], Loss: 0.0225\n",
      "Epoch [77/100], Loss: 0.0119\n",
      "Epoch [78/100], Loss: 0.0152\n",
      "Epoch [79/100], Loss: 0.0086\n",
      "Epoch [80/100], Loss: 0.0105\n",
      "Epoch [81/100], Loss: 0.0103\n",
      "Epoch [82/100], Loss: 0.0105\n",
      "Epoch [83/100], Loss: 0.0101\n",
      "Epoch [84/100], Loss: 0.0147\n",
      "Epoch [85/100], Loss: 0.0110\n",
      "Epoch [86/100], Loss: 0.0125\n",
      "Epoch [87/100], Loss: 0.0089\n",
      "Epoch [88/100], Loss: 0.0178\n",
      "Epoch [89/100], Loss: 0.0113\n",
      "Epoch [90/100], Loss: 0.0162\n",
      "Epoch [91/100], Loss: 0.0100\n",
      "Epoch [92/100], Loss: 0.0120\n",
      "Epoch [93/100], Loss: 0.0088\n",
      "Epoch [94/100], Loss: 0.0168\n",
      "Epoch [95/100], Loss: 0.0084\n",
      "Epoch [96/100], Loss: 0.0111\n",
      "Epoch [97/100], Loss: 0.0072\n",
      "Epoch [98/100], Loss: 0.0077\n",
      "Epoch [99/100], Loss: 0.0075\n",
      "Epoch [100/100], Loss: 0.0077\n",
      "Test Error: 0.009115325663948343\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['label'].squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "mean_loss = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['label'].squeeze()\n",
    "        outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        mean_loss.append(loss.item())\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "mean_loss = np.mean(np.array(mean_loss))\n",
    "print(f'Test Error: {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be98a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the two MLP networks\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define the main network that combines the two MLPs\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, middle_state_size, output_size, second_network_path=None):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        # The first MLP maps inputs to middle states\n",
    "        self.mlp1 = MLP(input_size, [64, 128], middle_state_size)  # Example hidden sizes\n",
    "\n",
    "        # The second MLP maps middle states to outputs\n",
    "        self.mlp2 = MLP(middle_state_size, [128, 64], output_size)  # Example hidden sizes\n",
    "\n",
    "        # Load the second network weights if provided\n",
    "        if second_network_path:\n",
    "            self.mlp2.load_state_dict(torch.load(second_network_path))\n",
    "            # Freeze the second network\n",
    "            for param in self.mlp2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        middle_state = self.mlp1(x)\n",
    "        output = self.mlp2(middle_state)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "middle_state_size = 20\n",
    "output_size = 10\n",
    "second_network_path = 'path_to_second_network.pth'  # Replace with actual path\n",
    "\n",
    "# Initialize the network\n",
    "my_network = MyNetwork(input_size, middle_state_size, output_size, second_network_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eab2eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbbc3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = joints.shape[1]\n",
    "hidden_sizes = [64, 128]\n",
    "output_size = pos.shape[1]\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3aac3f8d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5148\n",
      "Epoch [2/100], Loss: 0.4394\n",
      "Epoch [3/100], Loss: 0.3474\n",
      "Epoch [4/100], Loss: 0.3329\n",
      "Epoch [5/100], Loss: 0.2440\n",
      "Epoch [6/100], Loss: 0.2707\n",
      "Epoch [7/100], Loss: 0.1287\n",
      "Epoch [8/100], Loss: 0.1930\n",
      "Epoch [9/100], Loss: 0.1414\n",
      "Epoch [10/100], Loss: 0.1110\n",
      "Epoch [11/100], Loss: 0.0809\n",
      "Epoch [12/100], Loss: 0.1321\n",
      "Epoch [13/100], Loss: 0.1150\n",
      "Epoch [14/100], Loss: 0.0906\n",
      "Epoch [15/100], Loss: 0.0791\n",
      "Epoch [16/100], Loss: 0.0530\n",
      "Epoch [17/100], Loss: 0.0550\n",
      "Epoch [18/100], Loss: 0.0562\n",
      "Epoch [19/100], Loss: 0.0309\n",
      "Epoch [20/100], Loss: 0.0561\n",
      "Epoch [21/100], Loss: 0.0354\n",
      "Epoch [22/100], Loss: 0.0241\n",
      "Epoch [23/100], Loss: 0.0542\n",
      "Epoch [24/100], Loss: 0.0354\n",
      "Epoch [25/100], Loss: 0.0440\n",
      "Epoch [26/100], Loss: 0.0398\n",
      "Epoch [27/100], Loss: 0.0234\n",
      "Epoch [28/100], Loss: 0.0414\n",
      "Epoch [29/100], Loss: 0.0252\n",
      "Epoch [30/100], Loss: 0.0255\n",
      "Epoch [31/100], Loss: 0.0260\n",
      "Epoch [32/100], Loss: 0.0438\n",
      "Epoch [33/100], Loss: 0.0247\n",
      "Epoch [34/100], Loss: 0.0418\n",
      "Epoch [35/100], Loss: 0.0194\n",
      "Epoch [36/100], Loss: 0.0286\n",
      "Epoch [37/100], Loss: 0.0304\n",
      "Epoch [38/100], Loss: 0.0282\n",
      "Epoch [39/100], Loss: 0.0200\n",
      "Epoch [40/100], Loss: 0.0385\n",
      "Epoch [41/100], Loss: 0.0217\n",
      "Epoch [42/100], Loss: 0.0209\n",
      "Epoch [43/100], Loss: 0.0259\n",
      "Epoch [44/100], Loss: 0.0176\n",
      "Epoch [45/100], Loss: 0.0258\n",
      "Epoch [46/100], Loss: 0.0171\n",
      "Epoch [47/100], Loss: 0.0175\n",
      "Epoch [48/100], Loss: 0.0157\n",
      "Epoch [49/100], Loss: 0.0175\n",
      "Epoch [50/100], Loss: 0.0160\n",
      "Epoch [51/100], Loss: 0.0215\n",
      "Epoch [52/100], Loss: 0.0124\n",
      "Epoch [53/100], Loss: 0.0179\n",
      "Epoch [54/100], Loss: 0.0248\n",
      "Epoch [55/100], Loss: 0.0122\n",
      "Epoch [56/100], Loss: 0.0134\n",
      "Epoch [57/100], Loss: 0.0110\n",
      "Epoch [58/100], Loss: 0.0190\n",
      "Epoch [59/100], Loss: 0.0148\n",
      "Epoch [60/100], Loss: 0.0160\n",
      "Epoch [61/100], Loss: 0.0151\n",
      "Epoch [62/100], Loss: 0.0110\n",
      "Epoch [63/100], Loss: 0.0183\n",
      "Epoch [64/100], Loss: 0.0174\n",
      "Epoch [65/100], Loss: 0.0163\n",
      "Epoch [66/100], Loss: 0.0121\n",
      "Epoch [67/100], Loss: 0.0149\n",
      "Epoch [68/100], Loss: 0.0108\n",
      "Epoch [69/100], Loss: 0.0092\n",
      "Epoch [70/100], Loss: 0.0132\n",
      "Epoch [71/100], Loss: 0.0135\n",
      "Epoch [72/100], Loss: 0.0115\n",
      "Epoch [73/100], Loss: 0.0121\n",
      "Epoch [74/100], Loss: 0.0112\n",
      "Epoch [75/100], Loss: 0.0175\n",
      "Epoch [76/100], Loss: 0.0104\n",
      "Epoch [77/100], Loss: 0.0097\n",
      "Epoch [78/100], Loss: 0.0168\n",
      "Epoch [79/100], Loss: 0.0132\n",
      "Epoch [80/100], Loss: 0.0135\n",
      "Epoch [81/100], Loss: 0.0137\n",
      "Epoch [82/100], Loss: 0.0100\n",
      "Epoch [83/100], Loss: 0.0099\n",
      "Epoch [84/100], Loss: 0.0140\n",
      "Epoch [85/100], Loss: 0.0110\n",
      "Epoch [86/100], Loss: 0.0128\n",
      "Epoch [87/100], Loss: 0.0128\n",
      "Epoch [88/100], Loss: 0.0115\n",
      "Epoch [89/100], Loss: 0.0101\n",
      "Epoch [90/100], Loss: 0.0134\n",
      "Epoch [91/100], Loss: 0.0208\n",
      "Epoch [92/100], Loss: 0.0153\n",
      "Epoch [93/100], Loss: 0.0107\n",
      "Epoch [94/100], Loss: 0.0134\n",
      "Epoch [95/100], Loss: 0.0160\n",
      "Epoch [96/100], Loss: 0.0075\n",
      "Epoch [97/100], Loss: 0.0170\n",
      "Epoch [98/100], Loss: 0.0103\n",
      "Epoch [99/100], Loss: 0.0108\n",
      "Epoch [100/100], Loss: 0.0148\n",
      "Test Error: 0.011640029678505564\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['label'].squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "mean_loss = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['label'].squeeze()\n",
    "        outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        mean_loss.append(loss.item())\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "mean_loss = np.mean(np.array(mean_loss))\n",
    "print(f'Test Error: {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92563440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path = './model_weights/test_1.pth'\n",
    "\n",
    "# Save the model's state_dict to the specified path\n",
    "torch.save(model.state_dict(), model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ee3faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main network that combines the two MLPs\n",
    "class IK_Network(nn.Module):\n",
    "    def __init__(self, input_size, middle_state_size, output_size, second_network_path=None):\n",
    "        super(IK_Network, self).__init__()\n",
    "        # The first MLP maps inputs to middle states\n",
    "        self.mlp1 = MLP(input_size, [64, 128], middle_state_size)  # Example hidden sizes\n",
    "\n",
    "        # The second MLP maps middle states to outputs\n",
    "        self.mlp2 = MLP(middle_state_size, [64, 128], output_size)  # Example hidden sizes\n",
    "\n",
    "        # Load the second network weights if provided\n",
    "        if second_network_path:\n",
    "            self.mlp2.load_state_dict(torch.load(second_network_path))\n",
    "            # Freeze the second network\n",
    "            for param in self.mlp2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        middle_state = self.mlp1(x)\n",
    "        output = self.mlp2(middle_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51b88492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = pos.shape[1]\n",
    "middle_state_size = joints.shape[1]\n",
    "output_size = pos.shape[1]\n",
    "second_network_path = './model_weights/test_1.pth'  # Replace with actual path\n",
    "\n",
    "# Initialize the network\n",
    "my_network = IK_Network(input_size, middle_state_size, output_size, second_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26374a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7bc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd0ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bf0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61875257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586c6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d51c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_choice = \"3DoF-3R\"\n",
    "mode_choice = \"FK\"\n",
    "test_size = 0.2\n",
    "batch_size = 32\n",
    "FK_train_loader, FK_test_loader, input_size, output_size = data_loader(robot_choice, mode_choice, test_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a4ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input_size\n",
    "hidden_sizes = [64, 128]\n",
    "output_size = output_size\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "model = MLP(input_size, hidden_sizes, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "test_criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25542f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5458\n",
      "Epoch [2/100], Loss: 0.4456\n",
      "Epoch [3/100], Loss: 0.3679\n",
      "Epoch [4/100], Loss: 0.3171\n",
      "Epoch [5/100], Loss: 0.2665\n",
      "Epoch [6/100], Loss: 0.2131\n",
      "Epoch [7/100], Loss: 0.1704\n",
      "Epoch [8/100], Loss: 0.1410\n",
      "Epoch [9/100], Loss: 0.1175\n",
      "Epoch [10/100], Loss: 0.1045\n",
      "Epoch [11/100], Loss: 0.0934\n",
      "Epoch [12/100], Loss: 0.0809\n",
      "Epoch [13/100], Loss: 0.0721\n",
      "Epoch [14/100], Loss: 0.0638\n",
      "Epoch [15/100], Loss: 0.0568\n",
      "Epoch [16/100], Loss: 0.0517\n",
      "Epoch [17/100], Loss: 0.0478\n",
      "Epoch [18/100], Loss: 0.0452\n",
      "Epoch [19/100], Loss: 0.0423\n",
      "Epoch [20/100], Loss: 0.0404\n",
      "Epoch [21/100], Loss: 0.0378\n",
      "Epoch [22/100], Loss: 0.0357\n",
      "Epoch [23/100], Loss: 0.0344\n",
      "Epoch [24/100], Loss: 0.0335\n",
      "Epoch [25/100], Loss: 0.0333\n",
      "Epoch [26/100], Loss: 0.0312\n",
      "Epoch [27/100], Loss: 0.0297\n",
      "Epoch [28/100], Loss: 0.0286\n",
      "Epoch [29/100], Loss: 0.0280\n",
      "Epoch [30/100], Loss: 0.0274\n",
      "Epoch [31/100], Loss: 0.0273\n",
      "Epoch [32/100], Loss: 0.0270\n",
      "Epoch [33/100], Loss: 0.0266\n",
      "Epoch [34/100], Loss: 0.0261\n",
      "Epoch [35/100], Loss: 0.0259\n",
      "Epoch [36/100], Loss: 0.0243\n",
      "Epoch [37/100], Loss: 0.0246\n",
      "Epoch [38/100], Loss: 0.0246\n",
      "Epoch [39/100], Loss: 0.0239\n",
      "Epoch [40/100], Loss: 0.0239\n",
      "Epoch [41/100], Loss: 0.0236\n",
      "Epoch [42/100], Loss: 0.0231\n",
      "Epoch [43/100], Loss: 0.0224\n",
      "Epoch [44/100], Loss: 0.0224\n",
      "Epoch [45/100], Loss: 0.0216\n",
      "Epoch [46/100], Loss: 0.0212\n",
      "Epoch [47/100], Loss: 0.0213\n",
      "Epoch [48/100], Loss: 0.0207\n",
      "Epoch [49/100], Loss: 0.0203\n",
      "Epoch [50/100], Loss: 0.0203\n",
      "Epoch [51/100], Loss: 0.0204\n",
      "Epoch [52/100], Loss: 0.0206\n",
      "Epoch [53/100], Loss: 0.0197\n",
      "Epoch [54/100], Loss: 0.0197\n",
      "Epoch [55/100], Loss: 0.0189\n",
      "Epoch [56/100], Loss: 0.0187\n",
      "Epoch [57/100], Loss: 0.0193\n",
      "Epoch [58/100], Loss: 0.0174\n",
      "Epoch [59/100], Loss: 0.0180\n",
      "Epoch [60/100], Loss: 0.0177\n",
      "Epoch [61/100], Loss: 0.0170\n",
      "Epoch [62/100], Loss: 0.0168\n",
      "Epoch [63/100], Loss: 0.0171\n",
      "Epoch [64/100], Loss: 0.0166\n",
      "Epoch [65/100], Loss: 0.0185\n",
      "Epoch [66/100], Loss: 0.0164\n",
      "Epoch [67/100], Loss: 0.0163\n",
      "Epoch [68/100], Loss: 0.0158\n",
      "Epoch [69/100], Loss: 0.0159\n",
      "Epoch [70/100], Loss: 0.0155\n",
      "Epoch [71/100], Loss: 0.0150\n",
      "Epoch [72/100], Loss: 0.0152\n",
      "Epoch [73/100], Loss: 0.0150\n",
      "Epoch [74/100], Loss: 0.0162\n",
      "Epoch [75/100], Loss: 0.0155\n",
      "Epoch [76/100], Loss: 0.0154\n",
      "Epoch [77/100], Loss: 0.0146\n",
      "Epoch [78/100], Loss: 0.0139\n",
      "Epoch [79/100], Loss: 0.0138\n",
      "Epoch [80/100], Loss: 0.0150\n",
      "Epoch [81/100], Loss: 0.0143\n",
      "Epoch [82/100], Loss: 0.0132\n",
      "Epoch [83/100], Loss: 0.0138\n",
      "Epoch [84/100], Loss: 0.0134\n",
      "Epoch [85/100], Loss: 0.0130\n",
      "Epoch [86/100], Loss: 0.0131\n",
      "Epoch [87/100], Loss: 0.0124\n",
      "Epoch [88/100], Loss: 0.0124\n",
      "Epoch [89/100], Loss: 0.0121\n",
      "Epoch [90/100], Loss: 0.0121\n",
      "Epoch [91/100], Loss: 0.0122\n",
      "Epoch [92/100], Loss: 0.0129\n",
      "Epoch [93/100], Loss: 0.0132\n",
      "Epoch [94/100], Loss: 0.0120\n",
      "Epoch [95/100], Loss: 0.0119\n",
      "Epoch [96/100], Loss: 0.0119\n",
      "Epoch [97/100], Loss: 0.0110\n",
      "Epoch [98/100], Loss: 0.0114\n",
      "Epoch [99/100], Loss: 0.0111\n",
      "Epoch [100/100], Loss: 0.0108\n",
      "Test Error: 0.008065517969606887\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in FK_train_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['targets'].squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "mean_loss = []\n",
    "with torch.no_grad():\n",
    "    for batch in FK_test_loader:\n",
    "        inputs = batch['data']\n",
    "        labels = batch['targets'].squeeze()\n",
    "        outputs = model(inputs)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = test_criterion(outputs, labels)\n",
    "        mean_loss.append(loss.item())\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "mean_loss = np.mean(np.array(mean_loss))\n",
    "print(f'Test Error: {mean_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e089533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5218ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02affe0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
