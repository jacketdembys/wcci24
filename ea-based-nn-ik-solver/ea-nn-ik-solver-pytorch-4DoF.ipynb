{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13508c64-b766-4bae-9753-c277faab6f41",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37140ea7-36f6-4dcb-9dd3-7a0846b9e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2fc96-806d-41db-a30c-65e8784b0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sklearn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74138beb-8cb5-46ca-a079-e2ffb2354046",
   "metadata": {},
   "source": [
    "### Experiment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e9920-2b24-4c09-9f16-9124e82be285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "robot_choice = \"4DoF-2RPR\"\n",
    "seed = False                                                                   # seed random generators for reproducibility\n",
    "visualize_joints = True                                                       # visualize joint distribution in dataset     \n",
    "visualize_workspace = True                                                    # visualize workspace (positions)\n",
    "visualize_losses = True                                                       # visuallze training and validation losses\n",
    "visualize_normalized_workspace = True                                         # visualize normalized workspace (positions - debugging purposes)\n",
    "visualize_workspace_results = True                                            # visualize results in workapce\n",
    "print_inference_summary = True                                                # perform and print inference summary after training is done\n",
    "print_epoch = True                                                            # print epoch info during training and total training time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')         # device to train on\n",
    "batch_size = 100                                                              # desired batch size\n",
    "init_type = \"default\"                                                         # weights init method (default, uniform, normal, xavier_uniform, xavier_normal)\n",
    "hidden_layer_sizes = [128,128,128,128]                                           # architecture to employ\n",
    "learning_rate = 1e-4                                                          # learning rate\n",
    "optimizer_choice = \"SGD\"                                                      # optimizers (SGD, Adam, Adadelta, RMSprop)\n",
    "loss_choice = \"l2\"                                                            # l2, l1\n",
    "\n",
    "EPOCHS = 10000                                                                # total training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4c6b7-9cf7-40a6-95f3-2bf7625bc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if robot_choice == \"4DoF-2RPR\":\n",
    "    n_DoF = 4\n",
    "    input_dim = 3\n",
    "    output_dim = 4\n",
    "    \n",
    "data = pd.read_csv('data_'+robot_choice+'_2.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648196e1-0c68-4d98-abe4-aa74fcc835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize joints\n",
    "data_a = np.array(data) \n",
    "n_samples = data_a.shape[0]\n",
    "values = np.linspace(1,n_samples,n_samples)\n",
    "\n",
    "if visualize_joints:\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,4, figsize=(13, 4)) #, sharex=True, sharey=True)\n",
    "    axs[0].scatter(values, data_a[:,6],s=1, marker='o', c='r')\n",
    "    axs[0].set(xlabel='samples', ylabel='joint value', title='theta 1')\n",
    "    axs[1].scatter(values, data_a[:,7],s=1, marker='o', c='g')\n",
    "    axs[1].set(xlabel='samples', title='theta 2')\n",
    "    axs[2].scatter(values, data_a[:,8],s=1, marker='o', c='b')\n",
    "    axs[2].set(xlabel='samples', title='theta 3')\n",
    "    axs[3].scatter(values, data_a[:,9],s=1, marker='o', c='m')\n",
    "    axs[3].set(xlabel='samples', title='theta 4')\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axs = plt.subplots(1,4, figsize=(13, 4)) #, sharex=True, sharey=True)\n",
    "    axs[0].scatter(values, data_a[:,3],s=1, marker='o', c='r')\n",
    "    axs[0].set(xlabel='samples', ylabel='joint value', title='theta 1')\n",
    "    axs[1].scatter(values, data_a[:,4],s=1, marker='o', c='g')\n",
    "    axs[1].set(xlabel='samples', title='theta 2')\n",
    "    axs[2].scatter(values, data_a[:,5],s=1, marker='o', c='b')\n",
    "    axs[2].set(xlabel='samples', title='theta 3')\n",
    "    axs[3].scatter(values, data_a[:,6],s=1, marker='o', c='m')\n",
    "    axs[3].set(xlabel='samples', title='theta 4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f500c-a5ca-4a55-85a2-bb62e45242fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_workspace:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter(data_a[:,0], data_a[:,1], data_a[:,2], s=2, c='b', marker='.')\n",
    "    ax.scatter(0,0,0,s=20, marker='o', c='r')\n",
    "    ax.legend([\"robot position\",\"robot base\"])\n",
    "    ax.set(xlabel='x(m)', ylabel='y(m)', zlabel='z(m)',\n",
    "           title='Visualization of '+robot_choice+ ' dataset')\n",
    "    ax.view_init(60, 25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b194880-e491-4b32-b59a-684939972dce",
   "metadata": {},
   "source": [
    "### Utilities (Classes and Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719c61b-32ff-48b4-aa95-9a6e998615e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla MLP architecture\n",
    "class MLP_2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = \"MLP[]\"\n",
    "        self.input_fc = nn.Linear(input_dim, 250)\n",
    "        self.hidden_fc = nn.Linear(250, 100)\n",
    "        self.output_fc = nn.Linear(100, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = [batch size, height, width]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        # x = [batch size, height * width]\n",
    "        \n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "        # h_1 = [batch size, 250]\n",
    "\n",
    "        h_2 = F.relu(self.hidden_fc(h_1))\n",
    "        # h_2 = [batch size, 100]\n",
    "\n",
    "        y_pred = self.output_fc(h_2)\n",
    "        # y_pred = [batch size, output dim]\n",
    "\n",
    "        return y_pred, h_2\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, h_sizes, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.name = \"MLP [{}, {}, {}]\".format(str(input_dim), str(h_sizes).replace(\"[\",\"\").replace(\"]\",\"\"), str(output_dim))\n",
    "        self.input_dim = input_dim\n",
    "        self.h_sizes = h_sizes\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.input_fc = nn.Linear(self.input_dim, self.h_sizes[0])\n",
    "        \n",
    "        self.hidden_fc = nn.ModuleList()\n",
    "        for i in range(len(self.h_sizes)-1):\n",
    "            self.hidden_fc.append(nn.Linear(self.h_sizes[i], self.h_sizes[i+1]))\n",
    "        \n",
    "        self.output_fc = nn.Linear(self.h_sizes[len(self.h_sizes)-1], output_dim)\n",
    "\n",
    "        self.selu_activation = nn.SELU()\n",
    "        self.relu_activation = nn.ReLU()\n",
    "        self.prelu_activation = nn.PReLU()\n",
    "        self.lrelu_activation = nn.LeakyReLU()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        self.batch_norm_fc = nn.BatchNorm1d(20000)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.input_fc(x)\n",
    "        #x = self.batch_norm_fc(x)\n",
    "        x = self.relu_activation(x)  # ReLU(), Sigmoid(), LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        for i in range(len(self.h_sizes)-1):\n",
    "            x = self.hidden_fc[i](x)\n",
    "            #x = self.batch_norm_fc(x)\n",
    "            x = self.relu_activation(x)\n",
    "\n",
    "        x = self.output_fc(x)\n",
    "        x_temp = x\n",
    "\n",
    "        return x, x_temp \n",
    "\n",
    "\n",
    "# count network parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# data loader\n",
    "class LoadIKDataset(Dataset):\n",
    "    def __init__(self, inputs_array, outputs_array):\n",
    "        x_temp = inputs_array\n",
    "        y_temp = outputs_array\n",
    "\n",
    "        self.x_data = torch.tensor(x_temp, dtype=torch.float32) \n",
    "        self.y_data = torch.tensor(y_temp, dtype=torch.float32) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        current_input = self.x_data[idx, :]\n",
    "        current_output = self.y_data[idx, :]\n",
    "        \n",
    "        sample = {'input': current_input,\n",
    "                  'output': current_output}\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "\n",
    "# function to load the dataset\n",
    "def load_dataset(data, n_DoF, batch_size):\n",
    "\n",
    "    # file data_4DoF\n",
    "    #X = data[:,:3]\n",
    "    #y = data[:,6:]\n",
    "\n",
    "    # file data_4DOF_2\n",
    "    X = data[:,:3]\n",
    "    y = data[:,3:]\n",
    "\n",
    "        \n",
    "    #y = data[:,:2]\n",
    "    #X = data[:,2:]\n",
    "        \n",
    "    # split in train and test sets\n",
    "    X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X, \n",
    "                                                                y, \n",
    "                                                                test_size = 0.1,\n",
    "                                                                random_state = 1)\n",
    "\n",
    "    sc_in = MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
    "    sc_out = MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
    "    \n",
    "    X_train = sc_in.fit_transform(X_train_i)\n",
    "    X_test = sc_in.transform(X_test_i)  \n",
    "    #X_train = X_train_i\n",
    "    #X_test = X_test_i\n",
    "\n",
    "    #y_train = sc_out.fit_transform(y_train)\n",
    "    #y_test = sc_out.transform(y_test) \n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(y_train_i.shape)\n",
    "\n",
    "    train_data = LoadIKDataset(X_train, y_train_i)\n",
    "    test_data = LoadIKDataset(X_test, y_test_i)\n",
    "\n",
    "    train_data_loader = DataLoader(dataset=train_data,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=False)\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=test_data,\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False)\n",
    "\n",
    "    return train_data_loader, test_data_loader, X_test_i, y_test_i, X_train, y_train_i\n",
    "\n",
    "# train function\n",
    "def train(model, iterator, optimizer, criterion, batch_size, device, epoch, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    i = 0\n",
    "    \n",
    "    #with tqdm(total=(len(iterator) - len(iterator) % batch_size)) as t:\n",
    "    with tqdm(total=len(iterator), desc='Epoch: [{}/{}]'.format(epoch+1, EPOCHS), disable=True) as t:\n",
    "        for data in iterator:\n",
    "        #for data in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = data['input'], data['output']\n",
    "            #print(x)\n",
    "            #print(y)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred, _ = model(x)\n",
    "            \"\"\"\n",
    "            if i == 1:\n",
    "                print(\"\\nTrain Epoch {} at batch {}\".format(epoch, i))\n",
    "                print(y_pred[:5,:])\n",
    "                print(y[:5,:])\n",
    "                #sys.exit()\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            t.set_postfix_str('Train loss: {:.6f}'.format(epoch_loss/len(iterator)))\n",
    "            t.update()\n",
    "\n",
    "            i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"Total batches {}\".format(i))\n",
    "        \n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "# evaluation function \n",
    "def evaluate(model, iterator, criterion, device, epoch, EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #for data in tqdm(iterator, desc=\"Evaluating\", leave=False):        \n",
    "        with tqdm(total=len(iterator), desc='Epoch: [{}/{}]'.format(epoch+1, EPOCHS), disable=True) as t:\n",
    "            for data in iterator:\n",
    "                x = data['input'].to(device)\n",
    "                y = data['output'].to(device)\n",
    "                y_pred, _ = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                epoch_loss += loss.item()\n",
    "    \n",
    "                t.set_postfix_str('Valid loss: {:.6f}'.format(epoch_loss/len(iterator)))\n",
    "                t.update()\n",
    "\n",
    "    return epoch_loss/len(iterator)\n",
    "\n",
    "# make predictions\n",
    "def inference(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    y_preds = []\n",
    "    y_desireds = []\n",
    "    X_desireds = []\n",
    "    for data in iterator:\n",
    "        x = data['input'].to(device)\n",
    "        y = data['output'].to(device)\n",
    "        y_pred, _ = model(x)\n",
    "        y_preds.append(y_pred.detach().cpu().numpy().squeeze())\n",
    "        y_desireds.append(y.detach().cpu().numpy().squeeze())\n",
    "        #X_desireds.append(x.detach().cpu().numpy().squeeze())\n",
    "\n",
    "    y_desireds = np.array(y_desireds)\n",
    "    #X_desireds = np.array(X_desireds)\n",
    "    X_desireds = reconstruct_pose(y_desireds, robot_choice)\n",
    "    y_preds = np.array(y_preds)\n",
    "    X_preds = reconstruct_pose(y_preds, robot_choice)\n",
    "\n",
    "    X_errors = np.abs(X_preds- X_desireds)\n",
    "    y_errors = np.abs(y_preds- y_desireds)\n",
    "\n",
    "    X_errors_report = np.array([[X_errors.min(axis=0)],\n",
    "                                [X_errors.mean(axis=0)],\n",
    "                                [X_errors.max(axis=0)]]).squeeze()\n",
    "    \n",
    "    results = {\n",
    "        \"y_preds\": y_preds,\n",
    "        \"X_preds\": X_preds,\n",
    "        \"y_desireds\": y_desireds,\n",
    "        \"X_desireds\": X_desireds,\n",
    "        \"X_errors\": X_errors_report\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def inference_FK(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    y_preds = []\n",
    "    y_desireds = []\n",
    "    X_desireds = []\n",
    "    for data in iterator:\n",
    "        x = data['input'].to(device)\n",
    "        y = data['output'].to(device)\n",
    "        y_pred, _ = model(x)\n",
    "        y_preds.append(y_pred.detach().cpu().numpy().squeeze())\n",
    "        y_desireds.append(y.detach().cpu().numpy().squeeze())\n",
    "        X_desireds.append(x.detach().cpu().numpy().squeeze())\n",
    "\n",
    "    y_desireds = np.array(y_desireds)\n",
    "    X_desireds = np.array(X_desireds)\n",
    "    #X_desireds = reconstruct_pose(y_desireds, robot_choice)\n",
    "    y_preds = np.array(y_preds)\n",
    "    #X_preds = reconstruct_pose(y_preds, robot_choice)\n",
    "\n",
    "    #X_errors = np.abs(X_preds - X_desireds)\n",
    "    y_errors = np.abs(y_preds - y_desireds)\n",
    "\n",
    "    y_errors_report = np.array([[y_errors.min(axis=0)],\n",
    "                                [y_errors.mean(axis=0)],\n",
    "                                [y_errors.max(axis=0)]]).squeeze()\n",
    "    \n",
    "    results = {\n",
    "        \"y_preds\": y_preds,\n",
    "        #\"X_preds\": X_preds,\n",
    "        \"y_desireds\": y_desireds,\n",
    "        #\"X_desireds\": X_desireds,\n",
    "        \"y_errors\": y_errors_report\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# reconstruct positions in cartesian space from predictions\n",
    "def reconstruct_pose(y_preds, robot_choice):\n",
    "    y_preds = torch.from_numpy(y_preds)\n",
    "    n_samples = y_preds.shape[0]\n",
    "    pose = []\n",
    "    for i in range(n_samples):\n",
    "        t = y_preds[i,:]\n",
    "        DH = get_DH(robot_choice, t)\n",
    "        T = forward_kinematics(DH)\n",
    "        if robot_choice == \"4DoF-2RPR\":\n",
    "            # x,y,t1,t2,t3 where x,y (m) and t (rad)\n",
    "            pose.append(T[:3,-1].numpy())\n",
    "          \n",
    "    X_pred = np.array(pose)\n",
    "    return X_pred\n",
    "    \n",
    "\n",
    "# compute epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6a02b-adc8-48f1-b224-b417ba66c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = 1\n",
    "\n",
    "for experiment_number in range(experiments):\n",
    "\n",
    "    # ensure reproducibilities if seed is set to true\n",
    "    if seed:\n",
    "        SEED = 3    \n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    ## train and validate\n",
    "    # load the dataset\n",
    "    train_data_loader, test_data_loader, X_test, y_test, X_train, y_train = load_dataset(data_a, n_DoF, batch_size)\n",
    "    \n",
    "    # get network architecture\n",
    "    model = MLP(input_dim, hidden_layer_sizes, output_dim)\n",
    "    \n",
    "    if init_type == \"uniform\":\n",
    "        model.apply(weights_init_uniform_rule)\n",
    "    elif init_type == \"normal\":\n",
    "        model.apply(weights_init_normal_rule)\n",
    "    elif init_type == \"xavier_uniform\":\n",
    "        model.apply(weights_init_xavier_uniform_rule)\n",
    "    elif init_type == \"xavier_normal\":\n",
    "        model.apply(weights_init_xavier_normal_rule)\n",
    "    elif init_type == \"kaiming_uniform\":\n",
    "        model.apply(weights_init_kaiming_uniform_rule)\n",
    "    elif init_type == \"kaiming_normal\":\n",
    "        model.apply(weights_init_kaiming_normal_rule)\n",
    "        \n",
    "    model = model.to(device)\n",
    "    print(\"==> Architecture: {}\\n{}\".format(model.name, model))\n",
    "    print(\"==> Trainable parameters: {}\".format(count_parameters(model)))\n",
    "    \n",
    "    # set optimizer\n",
    "    if optimizer_choice == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif optimizer_choice == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "    elif optimizer_choice == \"Adadelta\":\n",
    "        optimizer = optim.Adadelta(model.parameters())\n",
    "    elif optimizer_choice == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters())\n",
    "    \n",
    "    # set loss\n",
    "    if loss_choice == \"l1\":\n",
    "        criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    elif loss_choice == \"l2\":\n",
    "        criterion = nn.L1Loss(reduction=\"mean\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n==> Experiment {} Training network: {}\".format(experiment_number+1, model.name))\n",
    "    \n",
    "    \n",
    "    # create a directory to save weights\n",
    "    save_path = robot_choice+\"_\" \\\n",
    "                +model.name.replace(\" \",\"\").replace(\"[\",\"_\").replace(\"]\",\"_\").replace(\",\",\"-\") \\\n",
    "                +optimizer_choice+\"_\" \\\n",
    "                +loss_choice+\"_\" \\\n",
    "                +str(experiment_number+1)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = float('inf')\n",
    "    start_time_train = time.monotonic()\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        start_time = time.monotonic()\n",
    "        train_loss = train(model, train_data_loader, optimizer, criterion, batch_size, device, epoch, EPOCHS)        \n",
    "        valid_loss = evaluate(model, test_data_loader, criterion, device, epoch, EPOCHS)\n",
    "    \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), save_path+'/best_epoch.pth')\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        end_time = time.monotonic()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if epoch % (EPOCHS/10) == 0 or epoch == EPOCHS-1:\n",
    "            if print_epoch:\n",
    "                print('Epoch: {}/{} | Epoch Time: {}m {}s'.format(epoch, EPOCHS, epoch_mins, epoch_secs))\n",
    "                print('\\tTrain Loss: {}'.format(train_loss))\n",
    "                print('\\tValid Loss: {}'.format(valid_loss))\n",
    "                print(\"\\tBest Epoch Occurred [{}/{}]\".format(best_epoch, EPOCHS))    \n",
    "            torch.save(model.state_dict(), save_path+'/epoch_'+str(epoch)+'.pth')\n",
    "    \n",
    "    end_time_train = time.monotonic()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time_train, end_time_train)\n",
    "    \n",
    "    if print_epoch:\n",
    "        print('\\nElapsed Time: {}m {}s'.format(epoch_mins, epoch_secs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388dcbb-3a4d-45e4-bf57-8036a6b4fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize losses\n",
    "if visualize_losses:\n",
    "    train_losses = np.array(train_losses)\n",
    "    valid_losses = np.array(valid_losses)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15,7))\n",
    "    ax.plot(train_losses, marker='.', c='b')\n",
    "    ax.plot(valid_losses, marker='.', c='r')\n",
    "    ax.set(xlabel='epochs', ylabel='losses',\n",
    "           title='Visualization of '+robot_choice+ ' losses')\n",
    "    ax.legend([\"training\", \"validation\"])\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c63d67-71a4-4a87-bbf3-ef21b830dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "if print_inference_summary:\n",
    "    weights_file = save_path+\"/best_epoch.pth\"\n",
    "    model = MLP(input_dim, hidden_layer_sizes, output_dim).to(device)\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    for n, p in torch.load(weights_file, map_location=lambda storage, loc: storage).items():\n",
    "        if n in state_dict.keys():\n",
    "            state_dict[n].copy_(p)\n",
    "        else:\n",
    "            raise KeyError(n)\n",
    "    \n",
    "    # get the results from training\n",
    "    \n",
    "    results = inference(model, test_data_loader, criterion, device)\n",
    "    y_preds = results[\"y_preds\"]\n",
    "    X_preds = results[\"X_preds\"]\n",
    "    y_desireds = results[\"y_desireds\"]\n",
    "    X_desireds = results[\"X_desireds\"]\n",
    "    X_errors = results[\"X_errors\"]\n",
    "    \n",
    "    print(\"==> y_preds {}: min = {:.4f} / mean = {:.4f} / max = {:.4f}\\n{}\\n\".format(y_preds.shape, y_preds.min(), y_preds.mean(), y_preds.max(), y_preds))\n",
    "    print(\"==> y_test {}: min = {:.4f} / mean = {:.4f} / max = {:.4f}\\n{}\\n\".format(y_test.shape, y_test.min(), y_test.mean(), y_test.max(), y_test))\n",
    "    \n",
    "    print(\"==> X_preds {}: min = {:.4f} / mean = {:.4f} / max = {:.4f}\\n{}\\n\".format(X_preds.shape, X_preds.min(), X_preds.mean(), X_preds.max(), X_preds))\n",
    "    print(\"==> X_test {}: min = {:.4f} / mean = {:.4f} / max = {:.4f}\\n{}\\n\".format(X_test.shape, X_test.min(), X_test.mean(), X_test.max(), X_test))\n",
    "    print(\"X_errors (mm): \\n{}\".format(X_errors*1000))\n",
    "\n",
    "    #results = inference_FK(model, test_data_loader, criterion, device)\n",
    "    #y_preds = results[\"y_preds\"]\n",
    "    #y_desireds = results[\"y_desireds\"]\n",
    "    #y_errors = results[\"y_errors\"]\n",
    "    \n",
    "    #print(\"==> y_preds {}: min = {:.4f} / mean = {:.4f} / max = {:.4f}\\n{}\\n\".format(y_preds.shape, y_preds.min(), y_preds.mean(), y_preds.max(), y_preds))\n",
    "    #print(\"==> y_test {}: min = {:.4f} / mean = {:.4f} / max = {:.4f}\\n{}\\n\".format(y_test.shape, y_test.min(), y_test.mean(), y_test.max(), y_test))\n",
    "    #print(\"y_errors (mm): \\n{}\".format(y_errors*1000))\n",
    "    \n",
    "    \n",
    "    X_errors_p = np.abs(X_preds - X_desireds)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad873f3-18a6-4595-8ebb-bdb55696f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_inference_summary:\n",
    "    # percentage of errors less than 1mm, 5mm, 10mm (1cm), 15mm (1.5cm), 20mm (2cm)\n",
    "    X_percentile = stats.percentileofscore(X_errors_p[:,0], [1,5,10,15,20], kind='rank')\n",
    "    Y_percentile = stats.percentileofscore(X_errors_p[:,1], [1,5,10,15,20], kind='rank')\n",
    "    Z_percentile = stats.percentileofscore(X_errors_p[:,2], [1,5,10,15,20], kind='rank')\n",
    "    print(\"X_percentile: {}\".format(X_percentile))\n",
    "    print(\"Y_percentile: {}\".format(Y_percentile))\n",
    "    print(\"Z_percentile: {}\".format(Z_percentile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a874a-84f8-4776-98c7-518d7502e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_workspace_results:\n",
    "    X_desireds = X_desireds*1000\n",
    "    X_preds = X_preds*1000    \n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter(X_desireds[:,0], X_desireds[:,1], X_desireds[:,2], s=2, c='b', marker='.')\n",
    "    ax.scatter(X_preds[:,0], X_preds[:,1], X_preds[:,2], s=2, c='r', marker='.')\n",
    "    ax.scatter(0,0,0,s=20, marker='o', c='lime')\n",
    "    ax.legend([\"desired\", \"predicted\", \"robot base\"])\n",
    "    ax.set(xlabel='x(m)', ylabel='y(m)', zlabel='z(m)',\n",
    "           title='Visualization of '+robot_choice+ ' workspace results')\n",
    "    ax.view_init(60, 25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88d08f-c98b-450d-8a2d-319df2c8277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_normalized_workspace:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], X_train[:,2], s=2, c='b', marker='.')\n",
    "    ax.scatter(0,0,0,s=20, marker='o', c='lime')\n",
    "    ax.legend([\"desired\", \"predicted\", \"robot base\"])\n",
    "    ax.set(xlabel='x(mm)', ylabel='y(mm)', zlabel='z(mm)',\n",
    "           title='Visualization of '+robot_choice+ ' normalized workspace')\n",
    "    ax.view_init(60, 25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cceb2-7c9f-4815-80a9-8b81855ea645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
