{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef510c3-4490-4e14-a0c6-c3c1635457d4",
   "metadata": {},
   "source": [
    "## Density Estimation Using Real NVP - Simple Tutorial\n",
    "\n",
    "From: https://github.com/xqding/RealNVP/blob/master/Real%20NVP%20Tutorial.ipynb\n",
    "\n",
    "The basic idea behind Real NVP is to construct a bijective transformation/flow between latent variables and observed variables such that the transformation satisfies:\n",
    "\n",
    "1. It is invertible and the determinant of its Jacobian matrix is easy to compute\n",
    "2. It is flexible such that it can transform random variables of simple distributions into random variables of complex distributions\n",
    "\n",
    "Other prominent implementations can be found here:\n",
    "\n",
    "1. https://github.com/fmu2/realNVP/tree/master\n",
    "2. https://github.com/kamenbliznashki/normalizing_flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f769f-7430-421d-a950-c7936ae1aabb",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aab4685-2236-44ae-9644-45a45d1288aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from sys import exit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as functional\n",
    "import time\n",
    "import os\n",
    "\n",
    "torch.set_default_dtype(torch.float64) #use double precision numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2707b561-5e5c-4400-a3c0-db14d97bf3be",
   "metadata": {},
   "source": [
    "### Implement Affine Coupling Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdd1d46-f7fe-469b-bd3a-acb1ccd72972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine_Coupling(nn.Module):\n",
    "    def __init__(self, mask, hidden_dim):\n",
    "        super(Affine_Coupling, self).__init__()\n",
    "        self.input_dim = len(mask)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## mask to separate positions that do not change and positions that change.\n",
    "        ## mask[i] = 1 means the ith position does not change.\n",
    "        self.mask = nn.Parameter(mask, requires_grad = False)\n",
    "\n",
    "        ## layers used to compute scale in affine transformation\n",
    "        self.scale_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.scale_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.scale_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "        self.scale = nn.Parameter(torch.Tensor(self.input_dim))\n",
    "        init.normal_(self.scale)\n",
    "\n",
    "        ## layers used to compute translation in affine transformation \n",
    "        self.translation_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.translation_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.translation_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "\n",
    "    def _compute_scale(self, x):\n",
    "        ## compute scaling factor using unchanged part of x with a neural network\n",
    "        x = x*self.mask\n",
    "        s = torch.relu(self.scale_fc1(x))\n",
    "        s = torch.relu(self.scale_fc2(s))\n",
    "        s = torch.relu(self.scale_fc3(s)) * self.scale        \n",
    "        return s\n",
    "\n",
    "    def _compute_translation(self, x):\n",
    "        ## compute translation using unchanged part of x with a neural network \n",
    "        x = x*self.mask       \n",
    "        t = torch.relu(self.translation_fc1(x))\n",
    "        t = torch.relu(self.translation_fc2(t))\n",
    "        t = self.translation_fc3(t)        \n",
    "        return t\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## convert latent space variable to observed variable\n",
    "        s = self._compute_scale(x)\n",
    "        t = self._compute_translation(x)\n",
    "        \n",
    "        y = self.mask*x + (1-self.mask)*(x*torch.exp(s) + t)        \n",
    "        logdet = torch.sum((1 - self.mask)*s, -1)\n",
    "        \n",
    "        return y, logdet\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed variable to latent space variable\n",
    "        s = self._compute_scale(y)\n",
    "        t = self._compute_translation(y)\n",
    "                \n",
    "        x = self.mask*y + (1-self.mask)*((y - t)*torch.exp(-s))\n",
    "        logdet = torch.sum((1 - self.mask)*(-s), -1)\n",
    "        \n",
    "        return x, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63dfd22-34f5-4c88-b6b0-fe4fa3eb4129",
   "metadata": {},
   "source": [
    "### Implement Real NVP by combining Affine Coupling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78684da7-9770-4544-9025-361cc94eb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP_2D(nn.Module):\n",
    "    '''\n",
    "    A vanilla RealNVP class for modeling 2-dimensional distributions\n",
    "    '''\n",
    "    def __init__(self, masks, hidden_dim):\n",
    "        '''\n",
    "        initialized with a list of masks. each mask defines an affine coupling layer\n",
    "        '''\n",
    "        super(RealNVP_2D, self).__init__()        \n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.masks = nn.ParameterList(\n",
    "            [nn.Parameter(torch.Tensor(m),requires_grad = False)\n",
    "             for m in masks])\n",
    "\n",
    "        self.affine_couplings = nn.ModuleList(\n",
    "            [Affine_Coupling(self.masks[i], self.hidden_dim)\n",
    "             for i in range(len(self.masks))])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Convert latent space variables into observed variables\n",
    "        y = x\n",
    "        logdet_tot = 0\n",
    "        for i in range(len(self.affine_couplings)):\n",
    "            y, logdet = self.affine_couplings[i](y)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "\n",
    "        ## A normalization layer is added such that the observed variables is within\n",
    "        ## the range of [-4, 4].\n",
    "        logdet = torch.sum(torch.log(torch.abs(4*(1-(torch.tanh(y))**2))), -1)        \n",
    "        y = 4*torch.tanh(y)\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "        \n",
    "        return y, logdet_tot\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed variables into latent space variables        \n",
    "        x = y        \n",
    "        logdet_tot = 0\n",
    "\n",
    "        # inverse the normalization layer\n",
    "        logdet = torch.sum(torch.log(torch.abs(1.0/4.0* 1/(1-(x/4)**2))), -1)\n",
    "        x  = 0.5*torch.log((1+x/4)/(1-x/4))\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "\n",
    "        ## inverse affine coupling layers\n",
    "        for i in range(len(self.affine_couplings)-1, -1, -1):\n",
    "            x, logdet = self.affine_couplings[i].inverse(x)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "            \n",
    "        return x, logdet_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21481fa-1346-46b0-b4f9-a2c884d1249f",
   "metadata": {},
   "source": [
    "### Fully Connected Real NVP - Possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bbdbfe-64bc-4505-8aeb-1523d682dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine_Coupling_NJ(nn.Module):\n",
    "    def __init__(self, mask, hidden_dim):\n",
    "        super(Affine_Coupling_NJ, self).__init__()\n",
    "        self.input_dim = len(mask)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## mask to separate positions that do not change and positions that change.\n",
    "        ## mask[i] = 1 means the ith position does not change.\n",
    "        self.mask = nn.Parameter(mask, requires_grad = False)\n",
    "\n",
    "        ## layers used to compute scale in affine transformation\n",
    "        self.scale_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.scale_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.scale_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "        self.scale = nn.Parameter(torch.Tensor(self.input_dim))\n",
    "        init.normal_(self.scale)\n",
    "\n",
    "        ## layers used to compute translation in affine transformation \n",
    "        self.translation_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.translation_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.translation_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "\n",
    "    def _compute_scale(self, x):\n",
    "        ## Compute scaling factor using unchanged part of x with a neural network\n",
    "        #x = x*self.mask\n",
    "        s = torch.relu(self.scale_fc1(x))\n",
    "        s = torch.relu(self.scale_fc2(s))\n",
    "        s = torch.relu(self.scale_fc3(s)) * self.scale        \n",
    "        return s\n",
    "\n",
    "    def _compute_translation(self, x):\n",
    "        ## Compute translation using the unchanged part of x with a neural network \n",
    "        x = x*self.mask       \n",
    "        t = torch.relu(self.translation_fc1(x))\n",
    "        t = torch.relu(self.translation_fc2(t))\n",
    "        t = self.translation_fc3(t)        \n",
    "        return t\n",
    "\n",
    "    def _compute_forward_numerical_jacobian(self, y, x, s, t):\n",
    "        ## Compute the full Jacobian numerically        \n",
    "        batch = y.shape[0]\n",
    "        nr = y.shape[1]\n",
    "        nc = x.shape[1]\n",
    "        J = torch.zeros(batch, nr, nc)\n",
    "        \n",
    "        #s = self._compute_scale(x)\n",
    "        #t = self._compute_translation(x)\n",
    "        \n",
    "        sp = 0.05 # small perturbation\n",
    "\n",
    "        for i in range(nc):\n",
    "            if i == 0:\n",
    "\n",
    "                #print(J.shape)\n",
    "                #print(s.shape)\n",
    "                J[:,0,0] = torch.exp(s[:,1])\n",
    "\n",
    "                x_left, x_right = x.clone(), x.clone()\n",
    "                x_left[:, 0], x_right[:, 0] = x_left[:, 0] - sp, x_right[:, 0] + sp                \n",
    "                s_left, s_right = self._compute_scale(x_left), self._compute_scale(x_right)\n",
    "                t_left, t_right = self._compute_translation(x_left), self._compute_translation(x_right) \n",
    "                \n",
    "                dsdx1 = (s_right[:,0] - s[:,0])/sp\n",
    "                dtdx1 = (t_right[:,0] - t[:,0])/sp\n",
    "                J[:,1,0] = x[:,1]*torch.exp(s[:,0])*dsdx1 + dtdx1\n",
    "\n",
    "            elif i == 1:\n",
    "\n",
    "                x_left, x_right = x.clone(), x.clone()\n",
    "                x_left[:, 1], x_right[:, 1] = x_left[:, 1] - sp, x_right[:, 1] + sp      \n",
    "                s_left, s_right = self._compute_scale(x_left), self._compute_scale(x_right)\n",
    "                t_left, t_right = self._compute_translation(x_left), self._compute_translation(x_right)\n",
    "                \n",
    "                dsdx2 = (s_right[:,1] - s[:,1])/sp\n",
    "                dtdx2 = (t_right[:,1] - t[:,1])/sp\n",
    "                J[:,0,1] = x[:,0]*torch.exp(s[:,1])*dsdx2 + dtdx2\n",
    "                \n",
    "                J[:,1,1] = torch.exp(s[:,0])\n",
    "        \n",
    "        return J\n",
    "\n",
    "    def _compute_inverse_numerical_jacobian(self, x, y, s, t):\n",
    "        ## compute the full jacobian numerically\n",
    "        #print(x.shape)\n",
    "        batch = x.shape[0]\n",
    "        nr = x.shape[1]\n",
    "        nc = y.shape[1]\n",
    "        J = torch.zeros(batch, nr, nc)\n",
    "        \n",
    "        #s = self._compute_scale(y)\n",
    "        #t = self._compute_translation(y)\n",
    "\n",
    "        sp = 0.05 # small perturbation\n",
    "        \n",
    "        for i in range(nc):\n",
    "            if i == 0:\n",
    "                \n",
    "                J[:,0,0] = torch.exp(-s[:,1])                \n",
    "\n",
    "                y_left, y_right = y.clone(), y.clone()\n",
    "                y_left[:, 0], y_right[:, 0] = y_left[:, 0] - sp, y_right[:, 0] + sp                \n",
    "                s_left, s_right = self._compute_scale(y_left), self._compute_scale(y_right)\n",
    "                t_left, t_right = self._compute_translation(y_left), self._compute_translation(y_right)  \n",
    "                \n",
    "                dsdy1 = (s_right[:,0] - s[:,0])/sp\n",
    "                dtdy1 = (t_right[:,0] - t[:,0])/sp        \n",
    "                \n",
    "                J[:,1,0] = torch.exp(-s[:,0]) * (-dtdy1 + (t[:,0] - y[:,1])*dsdy1) \n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                print(\"y: \", y[:5,:])\n",
    "                print(\"y_left: \", y_left[:5,:])\n",
    "                print(\"y_right: \", y_right[:5,:])\n",
    "                print()\n",
    "                print(\"s: \", s[:5,:])\n",
    "                print(\"s_left: \", s_left[:5,:])\n",
    "                print(\"s_right: \", s_right[:5,:])\n",
    "                print()\n",
    "                print(\"dsdy1: \", dsdy1.max())\n",
    "                print(\"dtdy1: \", dtdy1.max())\n",
    "                print()\n",
    "                \"\"\"\n",
    "                \n",
    "                \n",
    "            if i == 1:   \n",
    "\n",
    "                y_left, y_right = y.clone(), y.clone()\n",
    "                y_left[:, 1], y_right[:, 1] = y_left[:, 1] - sp, y_right[:, 1] + sp                \n",
    "                s_left, s_right = self._compute_scale(y_left), self._compute_scale(y_right)\n",
    "                t_left, t_right = self._compute_translation(y_left), self._compute_translation(y_right) \n",
    "                \n",
    "                dsdy2 = (s_right[:,1] - s[:,1])/sp\n",
    "                dtdy2 = (t_right[:,1] - t[:,1])/sp\n",
    "\n",
    "                J[:,0,1] = torch.exp(-s[:,1]) * (-dtdy2 + (t[:,1] - y[:,0])*dsdy2)\n",
    "        \n",
    "                J[:,1,1] = torch.exp(-s[:,0]) \n",
    "        \n",
    "        return J\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## convert latent space variable to observed variable\n",
    "        s = self._compute_scale(x)\n",
    "        t = self._compute_translation(x)\n",
    "        \n",
    "        y = self.mask*x + (1-self.mask)*(x*torch.exp(s) + t)        \n",
    "        logdet = torch.sum((1 - self.mask)*s, -1)\n",
    "\n",
    "        # compute the jacobian numerically\n",
    "        J = self._compute_forward_numerical_jacobian(y, x, s, t)\n",
    "        logdet_NJ = torch.log(torch.abs(torch.linalg.det(J.to(device))))\n",
    "        \n",
    "        return y, logdet, logdet_NJ\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed variable to latent space variable\n",
    "        s = self._compute_scale(y)\n",
    "        t = self._compute_translation(y)\n",
    "                \n",
    "        x = self.mask*y + (1-self.mask)*((y - t)*torch.exp(-s))\n",
    "        logdet = torch.sum((1 - self.mask)*(-s), -1)\n",
    "\n",
    "         # compute the jacobian numerically\n",
    "        J = self._compute_inverse_numerical_jacobian(x, y, s, t)\n",
    "        logdet_NJ = torch.log(torch.abs(torch.linalg.det(J.to(device))))\n",
    "\n",
    "        \n",
    "        #print(J[:5,:,:])\n",
    "        #print(\"J shape: \", J.shape)\n",
    "        #print(\"logdet_NJ : \", logdet_NJ[:5])\n",
    "        #print(\"logdet : \", logdet[:5])\n",
    "        #print(\"\\n\\n\")\n",
    "        \n",
    "        \n",
    "        return x, logdet, logdet_NJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23b4191-0c8d-4bad-8d98-f0d2a9e69358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP_2D_NJ(nn.Module):\n",
    "    '''\n",
    "    A vanilla RealNVP class for modeling 2 dimensional distributions with full numerical Jacobian\n",
    "    '''\n",
    "    def __init__(self, masks, hidden_dim):\n",
    "        '''\n",
    "        initialized with a list of masks. each mask define an affine coupling layer\n",
    "        '''\n",
    "        super(RealNVP_2D_NJ, self).__init__()        \n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.masks = nn.ParameterList(\n",
    "            [nn.Parameter(torch.Tensor(m),requires_grad = False)\n",
    "             for m in masks])\n",
    "\n",
    "        self.affine_couplings = nn.ModuleList(\n",
    "            [Affine_Coupling_NJ(self.masks[i], self.hidden_dim)\n",
    "             for i in range(len(self.masks))])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Convert latent space variables into observed variables\n",
    "        y = x\n",
    "        logdet_tot = 0\n",
    "        logdet_tot_NJ = 0\n",
    "        \n",
    "        for i in range(len(self.affine_couplings)):\n",
    "            y, logdet, logdet_NJ = self.affine_couplings[i](y)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "            logdet_tot_NJ = logdet_tot_NJ + logdet_NJ\n",
    "\n",
    "        ## A normalization layer is added such that the observed variables is within\n",
    "        ## the range of [-4, 4].\n",
    "        logdet = torch.sum(torch.log(torch.abs(4*(1-(torch.tanh(y))**2))), -1)        \n",
    "        y = 4*torch.tanh(y)\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "        logdet_tot_NJ = logdet_tot_NJ + logdet  \n",
    "        \n",
    "        return y, logdet_tot, logdet_tot_NJ\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## Convert observed variables into latent space variables        \n",
    "        x = y        \n",
    "        logdet_tot = 0 \n",
    "        logdet_tot_NJ = 0\n",
    "\n",
    "        # inverse the normalization layer\n",
    "        logdet = torch.sum(torch.log(torch.abs(1.0/4.0* 1/(1-(x/4)**2))), -1)\n",
    "        x  = 0.5*torch.log((1+x/4)/(1-x/4))\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "        logdet_tot_NJ = logdet_tot_NJ + logdet\n",
    "\n",
    "        ## inverse affine coupling layers\n",
    "        for i in range(len(self.affine_couplings)-1, -1, -1):\n",
    "            x, logdet, logdet_NJ = self.affine_couplings[i].inverse(x)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "            logdet_tot_NJ = logdet_tot_NJ + logdet_NJ\n",
    "            \n",
    "        return x, logdet_tot, logdet_tot_NJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2fe4f7-6714-4bf5-8033-dca1ce5efe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Architecture: \n",
      "None\n",
      "RealNVP_2D(\n",
      "  (masks): ParameterList(\n",
      "      (0): Parameter containing: [torch.float64 of size 2]\n",
      "      (1): Parameter containing: [torch.float64 of size 2]\n",
      "      (2): Parameter containing: [torch.float64 of size 2]\n",
      "      (3): Parameter containing: [torch.float64 of size 2]\n",
      "      (4): Parameter containing: [torch.float64 of size 2]\n",
      "      (5): Parameter containing: [torch.float64 of size 2]\n",
      "      (6): Parameter containing: [torch.float64 of size 2]\n",
      "      (7): Parameter containing: [torch.float64 of size 2]\n",
      "      (8): Parameter containing: [torch.float64 of size 2]\n",
      "      (9): Parameter containing: [torch.float64 of size 2]\n",
      "      (10): Parameter containing: [torch.float64 of size 2]\n",
      "      (11): Parameter containing: [torch.float64 of size 2]\n",
      "  )\n",
      "  (affine_couplings): ModuleList(\n",
      "    (0-11): 12 x Affine_Coupling(\n",
      "      (scale_fc1): Linear(in_features=2, out_features=128, bias=True)\n",
      "      (scale_fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (scale_fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "      (translation_fc1): Linear(in_features=2, out_features=128, bias=True)\n",
      "      (translation_fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (translation_fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==> Training:\n",
      "Epoch: 999, loss: 0.73963\n",
      "Epoch: 1999, loss: 0.51278\n",
      "Epoch: 2999, loss: 0.58652\n",
      "Epoch: 3999, loss: 0.45201\n",
      "Epoch: 4999, loss: 0.52768\n",
      "Epoch: 5999, loss: 0.41463\n",
      "Epoch: 6999, loss: 0.41509\n",
      "Epoch: 7999, loss: 0.41130\n",
      "Epoch: 8999, loss: 0.49172\n"
     ]
    }
   ],
   "source": [
    "## Seed random generators\n",
    "SEED = 3    \n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "## Masks used to define the number and the type of affine coupling layers\n",
    "## In each mask, 1 means that the variable at the corresponding position is\n",
    "## kept fixed in the affine coupling layer\n",
    "\"\"\"\n",
    "masks = [[1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0]]\n",
    "\"\"\"\n",
    "\n",
    "masks = [[1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0]]\n",
    "\n",
    "\n",
    "numExperiment = len(masks)/2\n",
    "\n",
    "## dimension of hidden units used in scale and translation transformation\n",
    "hidden_dim = 128\n",
    "\n",
    "## construct the RealNVP_2D object\n",
    "realNVP_type = \"analytical\" # analytical, numerical\n",
    "if realNVP_type == \"analytical\":\n",
    "    realNVP = RealNVP_2D(masks, hidden_dim)\n",
    "elif realNVP_type == \"numerical\":\n",
    "    realNVP = RealNVP_2D_NJ(masks, hidden_dim)\n",
    "\n",
    "print(print(\"==> Architecture: \"))\n",
    "print(realNVP)\n",
    "#exit()\n",
    "\n",
    "if torch.cuda.device_count():\n",
    "    realNVP = realNVP.cuda()\n",
    "device = next(realNVP.parameters()).device\n",
    "\n",
    "optimizer = optim.Adam(realNVP.parameters(), lr = 0.0001)\n",
    "\n",
    "EPOCHS = 10000\n",
    "# num_steps = 5000\n",
    "\n",
    "\n",
    "# save path for the results\n",
    "experiment_number = 0\n",
    "save_path = realNVP_type +\"_\"+str(experiment_number+1)\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "## the following loop learns the RealNVP_2D model by data\n",
    "## In each loop, data is dynamically sampled from the scipy moon dataset\n",
    "print(\"==> Training:\")\n",
    "best_valid_loss = float('inf')\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    ## sample data from the scipy moon dataset\n",
    "    X, label = datasets.make_moons(n_samples = 512, noise = 0.05)\n",
    "    X = torch.Tensor(X).to(device = device) \n",
    "    \n",
    "    if realNVP_type == \"analytical\":\n",
    "        ## transform data X to latent space Z\n",
    "        z, logdet = realNVP.inverse(X)\n",
    "\n",
    "        ## calculate the negative loglikelihood of X\n",
    "        loss = torch.log(z.new_tensor([2*math.pi])) + torch.mean(torch.sum(0.5*z**2, -1) - logdet)\n",
    "    \n",
    "        #print(\"loss: \", loss)\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "    elif realNVP_type == \"numerical\":\n",
    "        ## transform data X to latent space Z\n",
    "        z, logdet, logdet_NJ = realNVP.inverse(X)\n",
    "        \n",
    "        ## calculate the negative loglikelihood of X\n",
    "        loss = torch.log(z.new_tensor([2*math.pi])) + torch.mean(torch.sum(0.5*z**2, -1) - logdet_NJ)\n",
    "\n",
    "        #print(\"\\n\")\n",
    "        #print(\"loss: \", loss)\n",
    "        \n",
    "\n",
    "    #print(z.shape)\n",
    "    #print(z.new_tensor([2*math.pi]))\n",
    "    #print(logdet)\n",
    "    #print(torch.sum(0.5*z**2, -1))\n",
    "\n",
    "    \n",
    "    \n",
    "    #exit()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    #if (epoch + 1) % 1000 == 0:\n",
    "    if (epoch + 1) % (EPOCHS/10) == 0:\n",
    "    #if (epoch + 1) % 1 == 0:\n",
    "        print(f\"Epoch: {epoch:}, loss: {loss.item():.5f}\")\n",
    "        torch.save(realNVP.state_dict(), save_path+'/epoch_'+str(epoch)+'.pth')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time: {} seconds\".format(end-start))\n",
    "        \n",
    "## after learning, we can test if the model can transform\n",
    "## the moon data distribution into the normal distribution\n",
    "X, label = datasets.make_moons(n_samples = 1000, noise = 0.05)\n",
    "X = torch.Tensor(X).to(device = device)\n",
    "if realNVP_type == \"analytical\":\n",
    "    z, logdet_jacobian = realNVP.inverse(X)\n",
    "elif realNVP_type == \"numerical\":\n",
    "    z, logdet_jacobian, logdet_jacobian_NJ = realNVP.inverse(X)\n",
    "z = z.cpu().detach().numpy()\n",
    "\n",
    "X = X.cpu().detach().numpy()\n",
    "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
    "fig.clf()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(X[label==0,0], X[label==0,1], \".\")\n",
    "plt.plot(X[label==1,0], X[label==1,1], \".\")\n",
    "plt.title(\"X sampled from Moon dataset\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(z[label==0,0], z[label==0,1], \".\")\n",
    "plt.plot(z[label==1,0], z[label==1,1], \".\")\n",
    "plt.title(\"Z transformed from X\")\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "plt.savefig(\"exp_\"+str(int(numExperiment))+\"_layer_forward.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b2bba-1beb-428f-bb48-34494d82208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after learning, we can also test if the model can transform\n",
    "## the normal distribution into the moon data distribution \n",
    "if realNVP_type == \"analytical\":\n",
    "    realNVP_test = RealNVP_2D(masks, hidden_dim).to(device)\n",
    "elif realNVP_type == \"numerical\":\n",
    "    realNVP_test = RealNVP_2D_NJ(masks, hidden_dim).to(device)\n",
    "    \n",
    "weights_file = save_path+\"/epoch_9999.pth\"\n",
    "\n",
    "state_dict = realNVP_test.state_dict()\n",
    "for n, p in torch.load(weights_file, map_location=lambda storage, loc: storage).items():\n",
    "    if n in state_dict.keys():\n",
    "        state_dict[n].copy_(p)\n",
    "    else:\n",
    "        raise KeyError(n)\n",
    "\n",
    "\n",
    "\n",
    "SEED = 3    \n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "z = torch.normal(0, 1, size = (1000, 2)).to(device = device)\n",
    "if realNVP_type == \"analytical\":\n",
    "    X, _ = realNVP_test(z)\n",
    "elif realNVP_type == \"numerical\":\n",
    "    X, _, _ = realNVP_test(z)\n",
    "    \n",
    "X = X.cpu().detach().numpy()\n",
    "z = z.cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
    "fig.clf()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(z[:,0], z[:,1], \".\")\n",
    "plt.title(\"Z sampled from normal distribution\")\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(X[:,0], X[:,1], \".\")\n",
    "plt.title(\"X transformed from Z\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "\n",
    "plt.savefig(\"exp_\"+str(int(numExperiment))+\"_layer_inverse.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec525ec-2adc-492f-b900-0a92262b8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking determinant computational cost\n",
    "st = time.time()\n",
    "J = torch.rand(256,256)\n",
    "et = time.time()\n",
    "print(\"{} seconds.\".format(et-st))\n",
    "\n",
    "st = time.time()\n",
    "detJ = torch.linalg.det(J.to(device))\n",
    "et = time.time()\n",
    "print(\"{} seconds.\".format(et-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01779d13-9df4-4966-a6cb-8545dd1d774d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
